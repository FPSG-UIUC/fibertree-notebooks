{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run boilerplate code to set up environment\n",
    "\n",
    "%run ../prelude.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangular solver\n",
    "\n",
    "Based on scipy's [scipy.linalg.solve_triangular](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_triangular.html)\n",
    "\n",
    "Having a lower triangular matrix results in a _forward_ substitution, while an upper triangular matrix results in a _backward_ substitution.\n",
    "\n",
    "_Note:_ this was developed expecting uncompressed tensors but its use of `getPayload{,Ref}()` should support compressed formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = N = 8\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Generate a random upper triangular matrix A\n",
    "A_data = np.tril(np.random.randint(1, 6, size=(M, N)))\n",
    "\n",
    "# Generate the desired output vector x (Ax=b)\n",
    "X_ref_data = np.array(np.random.randint(1, 6, size=(N,)))\n",
    "\n",
    "# Evaluate the triangular matrix with X_ref to obtain B\n",
    "B_data = A_data.dot(X_ref_data)\n",
    "\n",
    "print(\"A:\\n\", A_data, \"\\nx (reference):\\n\", X_ref_data, \"\\nb:\\n\", B_data)\n",
    "\n",
    "assert(all(X_ref_data == scipy.linalg.solve_triangular(A_data, B_data, lower=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create uncompressed tensors\n",
    "A_MN = Tensor.fromUncompressed([\"M\", \"N\"], A_data.tolist()).setName(\"A\")\n",
    "B_N = Tensor.fromUncompressed([\"N\"], B_data.tolist()).setName(\"b\")\n",
    "X_ref = Tensor.fromUncompressed([\"M\"], X_ref_data.tolist()).setName(\"x_ref\")\n",
    "X = Tensor(rank_ids=[\"M\"]).setName(\"x\")\n",
    "\n",
    "A = A_MN.getRoot()\n",
    "\n",
    "inflate = False\n",
    "if inflate:\n",
    "    # hack to \"inflate\" A with coordinates where there are zeros,\n",
    "    # and allowing us to use square brackets (i.e. __getitem__)\n",
    "    # because positions == coordinates\n",
    "    A << Fiber(coords=range(M))\n",
    "    for j, a_j in A:\n",
    "        a_j << Fiber(coords=range(N))\n",
    "\n",
    "B = B_N.getRoot()\n",
    "\n",
    "displayTensor(A)\n",
    "displayTensor(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelined triangular solver\n",
    "\n",
    "`P` is a representation of pipelined communication; each row corresponds to the data passed from the previous stage to the next. Each row is N elements wide (same as B).\n",
    "\n",
    "This mapping corresponds to a pipeline of single-threaded compute engines (CEs) commensurate in size to the input.\n",
    "\n",
    "_Note:_ I traverse A in the N rank, then the M rank. I think I need a `swapRanks()` somewhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create \"pipes\" between each stage: N-entry tensors\n",
    "# (can't simply do a rank-2 tensor because I'd have multiple entries for addActivity for that tensor, and\n",
    "# current pipelining API doesn't really handle that\n",
    "#\n",
    "# for syntactic cleanliness I include B, the input tensor, in this array too\n",
    "BP = [B_N] + [Tensor(rank_ids=[\"N\"], shape=[N]).setName(f\"pipe-{m}\") for m in range(M)]\n",
    "\n",
    "canvas = createCanvas(A_MN, *BP, X, enable_wait=True)\n",
    "\n",
    "cycle = 0\n",
    "stage_delay = 1\n",
    "\n",
    "# \"wait\" means, for each key-value pair in the supplied dictionary,\n",
    "# we must wait `value` cycles when the `key`th argument in the canvas has been updated \n",
    "# in order to add our own activity\n",
    "wait = None # loop-carried variable\n",
    "\n",
    "for a_m in range(M):\n",
    "    # in the first row, read from B, rather than P\n",
    "    # also, skew these starts by cycle, as these input elements are piped in one after another\n",
    "    # m_ and n_activities provide pair-wise activity locations for adjacent stages in the pipeline\n",
    "    # e.g., [(m,), (m,), ()] then [(), (m,), (m,)] then [(), (), (m,)]\n",
    "    m_activities = [(a_m,) if mm == a_m or mm == a_m+1 else () for mm in range(M+1)]\n",
    "    # print(m_activities)\n",
    "\n",
    "    # process element on diagonal\n",
    "    curStage = BP[a_m]\n",
    "    nextStage = BP[a_m+1]\n",
    "    a_n = a_m\n",
    "    next_m = nextStage.getPayloadRef(a_m)\n",
    "    next_m <<= curStage.getPayload(a_m) / A.getPayload(a_m, a_n)\n",
    "    addActivity(canvas, (a_n, a_m), *m_activities, worker=str(a_m), skew=(cycle if a_m == 0 else 0), wait=wait)\n",
    "    cycle += 1\n",
    "    \n",
    "    # process elements off-diagonal on same row\n",
    "    # change from a_m to a_n (even though M=N) for clarity\n",
    "    for i_n in range(a_n+1, N):\n",
    "        n_activities = [(i_n,) if ii == a_n or ii == a_n+1 else () for ii in range(N+1)]\n",
    "        # print(n_activities)\n",
    "        \n",
    "        cur_n = curStage.getPayload(i_n)\n",
    "        next_n = nextStage.getPayloadRef(i_n)\n",
    "        next_n <<= cur_n - next_m * A.getPayload(i_n, a_m)\n",
    "\n",
    "        addActivity(canvas, (i_n, a_m), *n_activities, worker=str(a_m), skew=(cycle if a_m == 0 else 0), wait=wait)\n",
    "        cycle += 1\n",
    "        \n",
    "    # new values for next value of a_m\n",
    "    wait = {f\"pipe-{a_m}\": stage_delay}\n",
    "\n",
    "for x_m in range(M):\n",
    "    x_m_ref = X.getPayloadRef(x_m)\n",
    "    x_m_ref <<= BP[x_m+1].getPayload(x_m) # get m-th value from pipeline stage m\n",
    "    m_activities = [(x_m,) if mm == x_m else () for mm in range(M)]\n",
    "    # print(x_m, [], [], m_activities + [(x_m,)])\n",
    "    addActivity(canvas, [], [], *m_activities, (x_m,), wait={f\"pipe-{x_m}\":stage_delay})\n",
    "            \n",
    "displayCanvas(canvas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final result\n",
    "displayTensor(X)\n",
    "displayTensor(X_ref)\n",
    "X == X_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results\n",
    "\n",
    "Check the result by performing the dot product of `A` and `x` and ensuring that equals the `b` we started with. (The jth entry of `b` corresponds to the dot product of the jth _column_ of `A` and `x`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_check = Fiber(coords=range(N), initial=0)\n",
    "# displayTensor(B2)\n",
    "\n",
    "# canvas2 = createCanvas(A, X, B2)\n",
    "for j in range(N):\n",
    "    for i in range(N):\n",
    "        # print(j, i, a_pay[i], x_val, b2_ref)\n",
    "        b2_ref = B_check.getPayloadRef(j)\n",
    "        x_val = X.getPayload(i)\n",
    "        b2_ref += A.getPayload(j,i) * x_val \n",
    "        # addActivity(canvas2, (i,j), (i,), (i,))\n",
    "        \n",
    "# displayCanvas(canvas2)\n",
    "\n",
    "# check results\n",
    "B == B_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling schemes\n",
    "\n",
    "For your convenience, `A_MN` and `B_N` are repeated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayTensor(A_MN)\n",
    "displayTensor(B_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_MMNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_MMNN = A_MN.splitUniform(2).splitUniform(2, depth=2)\n",
    "displayTensor(A_MMNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_MNMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_MNMN = A_MMNN.swapRanks(depth=1)\n",
    "displayTensor(A_MNMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_NN = B_N.splitUniform(2)\n",
    "displayTensor(B_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
